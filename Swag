import pandas as pd
import re
from collections import Counter
from math import sqrt

def create_translation_table(df1, col1, df2, col2, threshold=0.9):
    def extract_stopwords(df, col, top_n=10):
        all_words = ' '.join(df[col]).lower().split()
        all_words = [re.sub(r'[^a-zA-Z0-9]', '', word) for word in all_words]
        common_words = [word for word, count in Counter(all_words).most_common(top_n)]
        return set(common_words)

    def normalize_name(name, stopwords):
        name = re.sub(r'[^a-zA-Z0-9\s]', '', name.lower())
        tokens = [word for word in name.split() if word not in stopwords]
        return ' '.join(tokens)

    def levenshtein_distance(a, b):
        if len(a) < len(b):
            return levenshtein_distance(b, a)
        if len(b) == 0:
            return len(a)
        previous_row = range(len(b) + 1)
        for i, c1 in enumerate(a):
            current_row = [i + 1]
            for j, c2 in enumerate(b):
                insertions = previous_row[j + 1] + 1
                deletions = current_row[j] + 1
                substitutions = previous_row[j] + (c1 != c2)
                current_row.append(min(insertions, deletions, substitutions))
            previous_row = current_row
        return previous_row[-1]

    def weighted_similarity(a, b, weights):
        a_tokens = set(a.split())
        b_tokens = set(b.split())
        common_tokens = a_tokens.intersection(b_tokens)
        token_sim = len(common_tokens) / max(len(a_tokens), len(b_tokens)) if max(len(a_tokens), len(b_tokens)) else 0

        common_chars = set(a).intersection(set(b))
        char_sim = len(common_chars) / max(len(a), len(b)) if max(len(a), len(b)) else 0

        intersection = len(a_tokens.intersection(b_tokens))
        union = len(a_tokens.union(b_tokens))
        jaccard_sim = intersection / union if union else 0

        distance = levenshtein_distance(a, b)
        levenshtein_sim = 1 - distance / max(len(a), len(b)) if max(len(a), len(b)) else 0

        all_tokens = list(set(a.split() + b.split()))
        a_vector = [a.split().count(token) for token in all_tokens]
        b_vector = [b.split().count(token) for token in all_tokens]
        dot_product = sum(a_v * b_v for a_v, b_v in zip(a_vector, b_vector))
        magnitude_a = sqrt(sum(a_v ** 2 for a_v in a_vector))
        magnitude_b = sqrt(sum(b_v ** 2 for b_v in b_vector))
        cosine_sim = dot_product / (magnitude_a * magnitude_b) if magnitude_a and magnitude_b else 0

        a_bigrams = set(a[i:i+2] for i in range(len(a) - 1))
        b_bigrams = set(b[i:i+2] for i in range(len(b) - 1))
        overlap = len(a_bigrams & b_bigrams)
        total = len(a_bigrams) + len(b_bigrams)
        dice_sim = 2 * overlap / total if total > 0 else 0

        return (weights['token'] * token_sim +
                weights['char'] * char_sim +
                weights['jaccard'] * jaccard_sim +
                weights['levenshtein'] * levenshtein_sim +
                weights['cosine'] * cosine_sim +
                weights['dice'] * dice_sim)

    def match_companies(df1, df2, weights, threshold):
        matches = []
        for name in df1['normalized_name']:
            potential_matches = []
            for candidate in df2['normalized_name']:
                score = weighted_similarity(name, candidate, weights)
                potential_matches.append((name, candidate, score))
            potential_matches.sort(key=lambda x: x[2], reverse=True)
            best_match = potential_matches[0] if potential_matches and potential_matches[0][2] >= threshold else (name, 'NA', 'NA')
            if best_match[2] != 'NA':
                matches.append(best_match)
        match_df = pd.DataFrame(matches, columns=['df1_company', 'df2_company', 'match_score'])
        return match_df

    stopwords_df1 = extract_stopwords(df1, col1, top_n=3)
    stopwords_df2 = extract_stopwords(df2, col2, top_n=3)
    combined_stopwords = stopwords_df1.union(stopwords_df2)

    df1['normalized_name'] = df1[col1].apply(lambda name: normalize_name(name, combined_stopwords))
    df2['normalized_name'] = df2[col2].apply(lambda name: normalize_name(name, combined_stopwords))

    weights = {
        'token': 0.2,
        'char': 0.15,
        'jaccard': 0.15,
        'levenshtein': 0.2,
        'cosine': 0.15,
        'dice': 0.15
    }

    matched_df = match_companies(df1, df2, weights, threshold)

    matched_df = matched_df.merge(df1, left_on='df1_company', right_on='normalized_name') \
                           .merge(df2, left_on='df2_company', right_on='normalized_name', suffixes=('_df1', '_df2'))

    result = matched_df[[col1 + '_df1', col2 + '_df2', 'match_score']]

    return result
result = create_translation_table(df1, 'company_name', df2, 'company_name', threshold=.5)
